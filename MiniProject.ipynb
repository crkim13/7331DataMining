{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project: SVM & LR Classification\n",
    "### Ryan Bass, Brett Benefield, Cho Kim, Nicole Wittlin\n",
    "\n",
    "<span style=\"color: blue\">Remove all blue text before submitting</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True) # one of the many color mappings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pandas.plotting import scatter_matrix\n",
    "from IPython.display import HTML, display\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectPercentile, SelectKBest, chi2, f_classif\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics as mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d\n",
    "def plot_coefficients(classifier, feature_names, top_features=20):\n",
    "    coef = classifier.coef_.ravel()\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "    # create plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    colors = [\"red\" if c < 0 else \"blue\" for c in coef[top_coefficients]]\n",
    "    plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1, 1 + 2 * top_features), feature_names[top_coefficients], rotation=60, ha=\"right\")\n",
    "    plt.show()\n",
    "    \n",
    "def getTopCoefficients(classifier, feature_names, top_features=20):\n",
    "    coef = classifier.coef_.ravel()\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "    feature_names = np.array(feature_names)\n",
    "    return feature_names[top_coefficients]\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/39812885/retain-feature-names-after-scikit-feature-selection\n",
    "def percentile_threshold_selector(data, percent=10):\n",
    "    selector = SelectPercentile(f_classif, percentile = percent)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "def scale_data(data):\n",
    "    temp = scaler.fit(data)\n",
    "    data = pd.DataFrame(temp, columns = data.columns)\n",
    "    return data\n",
    "\n",
    "# https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n",
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "verboseStatus = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\sandbox\\SMU\\dataMining\\choNotebook\\EducationDataNC\\2017\\Machine Learning Datasets\n",
      "********* Initial Values Before Cleaning *******************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 470 entries, 0 to 469\n",
      "Columns: 403 entries, student_num to unit_code\n",
      "dtypes: float64(322), int64(81)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Brett's directory\n",
    "# Desktop\n",
    "# %cd \"C:\\Sandbox\\SMU\\dataMining\\ChoRepo\\EducationDataNC\\2017\\Machine Learning Datasets\"\n",
    "# Laptop\n",
    "%cd \"C:\\sandbox\\SMU\\dataMining\\choNotebook\\EducationDataNC\\2017\\Machine Learning Datasets\"\n",
    "\n",
    "# Ryan's directory\n",
    "#%cd \"C:\\Users\\Clovis\\Documents\\7331DataMining\\EducationDataNC\\2017\\Machine Learning Datasets\"\n",
    "\n",
    "# Cho's directory. Either uncomment the cd statement above or make your own cd.\n",
    "#%cd \"/Users/chostone/Documents/Data Mining/7331DataMining/EducationDataNC/2017/Machine Learning Datasets\"\n",
    "\n",
    "# NW directorY \n",
    "#%cd \"C:\\Users\\Nicole Wittlin\\Documents\\7331DataMining\\EducationDataNC\\2017\\Raw Datasets\"\n",
    "dfPublicHS = pd.read_csv(\"PublicHighSchools2017_ML.csv\")\n",
    "\n",
    "print('********* Initial Values Before Cleaning *******************')\n",
    "dfPublicHS.info(verbose = verboseStatus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns Dropped: 1\n"
     ]
    }
   ],
   "source": [
    "# Credit: https://www.kaggle.com/saravanann/santander\n",
    "# Drop any columns that have zero variance\n",
    "totalColNum = dfPublicHS.shape[1]\n",
    "sel = VarianceThreshold()\n",
    "sel.fit(dfPublicHS)\n",
    "\n",
    "keepCols = sel.get_support(indices=True)\n",
    "dfColIndex = np.arange(dfPublicHS.columns.size)\n",
    "dropCol = np.delete(dfColIndex, keepCols)\n",
    "dfPublicHS = dfPublicHS.drop(dfPublicHS.columns[dropCol], axis = 1)\n",
    "\n",
    "print(\"Columns Dropped: {}\".format(len(dropCol)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "dfCorr = dfPublicHS.corr('pearson').abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dropped: 124\n"
     ]
    }
   ],
   "source": [
    "## source for code below: https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/ ##\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = dfCorr.where(np.triu(np.ones(dfCorr.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.8\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "\n",
    "# Drop Columns\n",
    "dfDropped = dfPublicHS.drop(columns = to_drop, axis = 1)\n",
    "\n",
    "print(\"Columns dropped: {}\".format(len(to_drop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 of 470: 50.0%\n"
     ]
    }
   ],
   "source": [
    "# Evenly split our target variable (predict whether a school will have 52% postsecondary enrollment)\n",
    "splitLimit = dfDropped['ALL_All Students (Total or Subtotal_ENROLL_sch_pct'].median()\n",
    "\n",
    "dfDropped['schoolPctTarget'] = np.where(dfDropped['ALL_All Students (Total or Subtotal_ENROLL_sch_pct'] >= splitLimit, 1, 0)\n",
    "\n",
    "print(\"{} of {}: {}%\".format(dfDropped['schoolPctTarget'].sum(), dfDropped.shape[0], \n",
    "                             round(dfDropped['schoolPctTarget'].sum()/dfDropped.shape[0] * 100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dfDropped['schoolPctTarget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop these columns because they contain the answer we are trying to predict\n",
    "dropCols = dfDropped.filter(regex = r'sch_pct|college')\n",
    "\n",
    "# Drop columns\n",
    "dfDropped.drop(dropCols, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=10, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxCols = dfDropped.shape[1]\n",
    "X = dfDropped.iloc[:,0:maxCols-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test and train dataset with an 80/20 split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X.astype(np.float), y, test_size = 0.2)\n",
    "\n",
    "# Scale data after splitting out test data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(xTrain)\n",
    "xTrain = scaler.transform(xTrain)\n",
    "xTest = scaler.transform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell needs to be moved but for now I'm going to put it here\n",
    "xTrainLR = xTrain\n",
    "xTestLR = xTest\n",
    "yTrainLR = yTrain\n",
    "yTestLR = yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5319148936170213\n",
      "confusion matrix\n",
      " [[32 21]\n",
      " [23 18]]\n",
      "Accuracy: 0.65 (+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l1', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "#iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "#for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "#    X_train = X[train_indices]\n",
    "#    y_train = y[train_indices]\n",
    "    \n",
    "#    X_test = X[test_indices]\n",
    "#    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "lr_clf.fit(X, y)  # train object\n",
    "y_hat = lr_clf.predict(xTest) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "acc = mt.accuracy_score(yTest,y_hat)\n",
    "conf = mt.confusion_matrix(yTest,y_hat)\n",
    "print(\"accuracy\", acc )\n",
    "print(\"confusion matrix\\n\",conf)\n",
    "\n",
    "scores = cross_val_score(lr_clf, X, y, cv=cv_object)\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7127659574468085\n",
      "[[34 19]\n",
      " [ 8 33]]\n",
      "GraduationRate_4yr_SWD has weight of -0.297633330345251\n",
      "TwoOrMoreMalePct has weight of -0.28421236915449377\n",
      "PacificIslandFemalePct has weight of -0.2828674859753109\n",
      "TwoOrMoreFemalePct has weight of -0.2517219118151226\n",
      "lea_advance_dgr_pct has weight of -0.2428376030838324\n",
      "tchyrs_4thru10_pct has weight of -0.23229327127108967\n",
      "GraduationRate_4yr_Hispanic has weight of -0.22651803890482997\n",
      "GraduationRate_5yr_EDS has weight of -0.2199120294293765\n",
      "lea_instruct_equip_exp_pct has weight of -0.207344656291715\n",
      "cte_courses has weight of -0.20441991378434313\n",
      "pct_eds has weight of -0.18904915624121638\n",
      "AsianFemalePct has weight of -0.18722441957960884\n",
      "wap_per_classroom has weight of -0.1801671369763713\n",
      "grade_range_cd_PK-12 has weight of -0.17813446322459361\n",
      "ACT_pTarget_PctMet has weight of -0.17438051627637213\n",
      "GraduationRate_4yr_LEP has weight of -0.17299835932562804\n",
      "calendar_type_txt_Regular School, Year-Round Calendar has weight of -0.16730614771211452\n",
      "grade_range_cd_8-12 has weight of -0.16578691481127983\n",
      "GraduationRate_4yr_Asian has weight of -0.16323595736527016\n",
      "EOCEnglish2_CACR_White has weight of -0.15529329802664116\n",
      "MathGr10_pTarget_PctMet has weight of -0.15140737877434607\n",
      "EOCEnglish2_CACR_Black has weight of -0.1476617360860203\n",
      "lea_services_expense_pct has weight of -0.14535014807628058\n",
      "long_susp_per_c_num has weight of -0.14371537725584344\n",
      "EOCEnglish2_CACR_LEP has weight of -0.14264612593469964\n",
      "lea_wap_num has weight of -0.13845390060262144\n",
      "Not Demostrated_TCHR_Standard 4_Pct has weight of -0.13790103885281532\n",
      "books_per_student has weight of -0.13724416015488208\n",
      "grades_BYOD_12 has weight of -0.13615062413971057\n",
      "SBE District_Southwest has weight of -0.13446713068125946\n",
      "grades_1_to_1_access_6:07:08 has weight of -0.12792546004087665\n",
      "lea_crime_per_c_num has weight of -0.1260619428758183\n",
      "ACTCompositeScore_UNCMin_Black has weight of -0.12600868529356846\n",
      "HispanicFemalePct has weight of -0.12308672242555874\n",
      "Accomplished_TCHR_Standard 5_Pct has weight of -0.12173180392710925\n",
      "lea_local_perpupil_num has weight of -0.11788347520052343\n",
      "EOCEnglish2_CACR_SWD has weight of -0.11566220595478209\n",
      "lea_sat_participation_pct has weight of -0.11275769684277981\n",
      "crime_per_c_num has weight of -0.11240147480227443\n",
      "EOCBiology_CACR_TwoorMoreRaces has weight of -0.1098255884399335\n",
      "ACTWorkKeys_SilverPlus_TwoorMoreRaces has weight of -0.10897063456614602\n",
      "grades_BYOD_PK:9:10:11:12 has weight of -0.10559270644669311\n",
      "ACTSubtests_BenchmarksMet_AmericanIndian has weight of -0.10487753296380527\n",
      "grades_1_to_1_access_11:12 has weight of -0.09855244602882943\n",
      "grades_BYOD_8:9:10:11:12 has weight of -0.09662187898018447\n",
      "Reading SPG Grade_C has weight of -0.09613667041847669\n",
      "grades_1_to_1_access_9:10:11:12:13 has weight of -0.09591514806755717\n",
      "stud_internet_comp_num has weight of -0.0953930436905948\n",
      "grades_1_to_1_access_9 has weight of -0.08927755284274906\n",
      "ARCH_Concentrator_Ct has weight of -0.08364568525890152\n",
      "lea_cte_courses has weight of -0.08110845354998832\n",
      "lea_long_susp_per_c_num has weight of -0.07918919653782282\n",
      "09_Size has weight of -0.076098769415006\n",
      "Proficient_TCHR_Standard 1_Pct has weight of -0.07599690871271193\n",
      "GraduationRate_4yr_EDS has weight of -0.0751002708368587\n",
      "lea_total_expense_num has weight of -0.07320079771758198\n",
      "grade_range_cd_7-12 has weight of -0.0730202716620493\n",
      "lea_tchyrs_0thru3_pct has weight of -0.07245879191361417\n",
      "SRC_Grades_Devices_Sent_Home_9:10:12 has weight of -0.07172790782465566\n",
      "grades_BYOD_6:7:8:9:10:11:12 has weight of -0.07166026930324015\n",
      "EOCSubjects_CACR_AIG has weight of -0.0712373659080437\n",
      "grade_range_cd_9-9 has weight of -0.06801876116251669\n",
      "grades_BYOD_9:10:11 has weight of -0.06799989388575613\n",
      "grade_range_cd_9-11 has weight of -0.0669819902878898\n",
      "_1_to_1_access_Yes has weight of -0.06681170554041564\n",
      "lea_books_per_student has weight of -0.06461977321949407\n",
      "Developing_TCHR_Standard 4_Pct has weight of -0.06418466622490185\n",
      "EOCBiology_CACR_White has weight of -0.061400215141240125\n",
      "SBE District_Piedmont-Triad has weight of -0.06048489834109014\n",
      "esea_status_P has weight of -0.0597426050583751\n",
      "st_avg_daily_attend_pct has weight of -0.05809597534515149\n",
      "grades_1_to_1_access_9:10 has weight of -0.05745517879946982\n",
      "grades_1_to_1_access_6:7:8:9:10:11:12:13 has weight of -0.056647978655590035\n",
      "lea_nbpts_num has weight of -0.055141135559367366\n",
      "HispanicMalePct has weight of -0.05279535750364609\n",
      "EOCMathI_CACR_LEP has weight of -0.04912380927183919\n",
      "ACTSubtests_BenchmarksMet_SWD has weight of -0.04872368793972211\n",
      "lea_supplies_expense_pct has weight of -0.048627899657413584\n",
      "grade_range_cd_6-12 has weight of -0.04858877290891112\n",
      "GraduationRate_5yr_LEP has weight of -0.048072997111360816\n",
      "10+ Years_LEA_Exp_Pct_Prin has weight of -0.04784312001659906\n",
      "Exceeds Expected Growth_TCHR_Student Growth_Pct has weight of -0.04738458446675406\n",
      "Accomplished_TCHR_Standard 2_Pct has weight of -0.045056767690901556\n",
      "lea_ap_participation_pct has weight of -0.045003124558435234\n",
      "EOCBiology_CACR_EDS has weight of -0.04488758730269651\n",
      "GraduationRate_4yr_TwoorMoreRaces has weight of -0.04433467449931932\n",
      "EOCSubjects_CACR_Hispanic has weight of -0.04422256241001737\n",
      "GraduationRate_5yr_Hispanic has weight of -0.041924065433309364\n",
      "ACTSubtests_BenchmarksMet_AIG has weight of -0.04165406147773555\n",
      "grades_1_to_1_access_9:10:11 has weight of -0.04136803782833532\n",
      "SRC_Grades_Devices_Sent_Home_10:11:12:13 has weight of -0.04135331898315327\n",
      "BMA_Concentrator_Ct has weight of -0.040866610810458184\n",
      "EOCSubjects_CACR_LEP has weight of -0.04016680378005184\n",
      "11_Size has weight of -0.039394346200654594\n",
      "grades_BYOD_11:12:13 has weight of -0.03869135438881021\n",
      "grades_1_to_1_access_10:11:12 has weight of -0.03841623857964202\n",
      "12_Size has weight of -0.03699575487249221\n",
      "grade_range_cd_9-13 has weight of -0.03667112284516881\n",
      "Number_Industry_Recognized_Crede has weight of -0.03410919857603912\n",
      "Math SPG Grade_C has weight of -0.03352483397538336\n",
      "NC Math 1 Score has weight of -0.03267692818706519\n",
      "Biology_Size has weight of -0.030681933269423364\n",
      "TRAN_Concentrator_Ct has weight of -0.029962513919899365\n",
      "lea_flicensed_teach_pct has weight of -0.029413978947398535\n",
      "grades_1_to_1_access_10:11:12:13 has weight of -0.02915922778093359\n",
      "EVAAS Growth Status_Met has weight of -0.02912201124843057\n",
      "grade_range_cd_7-13 has weight of -0.029105179320590934\n",
      "ACTCompositeScore_UNCMin_SWD has weight of -0.028735129790203925\n",
      "EOCMathI_CACR_SWD has weight of -0.023960879094747575\n",
      "Reading SPG Grade_D has weight of -0.02297403745313411\n",
      "Not Demostrated_TCHR_Standard 1_Pct has weight of -0.02241941843857831\n",
      "CurrentYearEOC_pTarget_PctMet has weight of -0.02158761402620278\n",
      "INFO_Concentrator_Ct has weight of -0.02152372604198861\n",
      "Does Not Meet Expected Growth_TCHR_Student Growth_Pct has weight of -0.0208916665969376\n",
      "STEM_Concentrator_Ct has weight of -0.01912706211266117\n",
      "nbpts_num has weight of -0.018807655057829217\n",
      "SPG Grade_C has weight of -0.016212334054869854\n",
      "ACT WorkKeys Score has weight of -0.014898223099717979\n",
      "ACTWorkKeys_pTarget_PctMet has weight of -0.013638717943095216\n",
      "EOCSubjects_CACR_SWD has weight of -0.013441038808258213\n",
      "Meets Expected Growth_TCHR_Student Growth_Pct has weight of -0.012662284776445567\n",
      "st_crime_per_c_num has weight of -0.011440269095646818\n",
      "grades_BYOD_9 has weight of -0.011268934748734867\n",
      "ACTSubtests_BenchmarksMet_Black has weight of -0.010511298951358848\n",
      "advance_dgr_pct has weight of -0.008483158763391696\n",
      "EOCBiology_CACR_LEP has weight of -0.008034156838502511\n",
      "ACTWorkKeys_SilverPlus_SWD has weight of -0.00789981709574307\n",
      "Distinguished_TCHR_Standard 1_Pct has weight of -0.007052848411922805\n",
      "avg_age_media_collection has weight of -0.006161249752977263\n",
      "grade_range_cd_3-12 has weight of -0.005210381721679715\n",
      "Not Demostrated_TCHR_Standard 3_Pct has weight of -0.005205467643517574\n",
      "short_susp_per_c_num has weight of -0.003954371352959185\n",
      "GraduationRate_4yr_Black has weight of -0.0035855968507830694\n",
      "digital_media_pct has weight of -0.0026563413699119626\n",
      "EOCMathI_CACR_TwoorMoreRaces has weight of -0.002435565965272437\n",
      "English II Score has weight of -0.0005126172122461984\n",
      "grades_1_to_1_access_11 has weight of 0.0\n",
      "grades_BYOD_9:11:12 has weight of 0.0\n",
      "lea_tchyrs_11plus_pct has weight of 0.0005224407889231546\n",
      "GraduationRate_4yr_AIG has weight of 0.0013684350223158157\n",
      "4-10 Years_LEA_Exp_Pct_Prin has weight of 0.0028538235757772606\n",
      "Developing_TCHR_Standard 2_Pct has weight of 0.0035680362199287034\n",
      "Passing NC Math 3 has weight of 0.003943335466210386\n",
      "lea_ap_pct_3_or_above has weight of 0.0048073694619728095\n",
      "grades_1_to_1_access_11:12:13 has weight of 0.0049403290205773795\n",
      "grades_BYOD_9:10:11:12 has weight of 0.005450759868675753\n",
      "Reading SPG Grade_B has weight of 0.005595233312122017\n",
      "grade_range_cd_6-13 has weight of 0.006927618595175244\n",
      "SciGr11_pTarget_PctMet has weight of 0.0070101290956399\n",
      "HLTH_Concentrator_Ct has weight of 0.007386472265321232\n",
      "Byod_Yes has weight of 0.008028449114664672\n",
      "lea_emer_prov_teach_pct has weight of 0.008437720517762329\n",
      "Developing_TCHR_Standard 5_Pct has weight of 0.009655599857787488\n",
      "SPG Grade_D has weight of 0.01260363262892542\n",
      "PacificIslandMalePct has weight of 0.014546306221350398\n",
      "EOCMathI_CACR_Hispanic has weight of 0.015131853519653862\n",
      "flicensed_teach_pct has weight of 0.015642558500097905\n",
      "total_specialized_courses has weight of 0.018688180600993958\n",
      "GraduationRate_5yr_AmericanIndian has weight of 0.020887022462444893\n",
      "SPG Grade_B has weight of 0.021027910746574844\n",
      "lea_federal_perpupil_num has weight of 0.02408389681973241\n",
      "student_num has weight of 0.0252204918236323\n",
      "lea_sat_avg_score_num has weight of 0.026473326483247107\n",
      "0-3 Years_LEA_Exp_Pct_Prin has weight of 0.02681411775433924\n",
      "grades_1_to_1_access_9:11:12:13 has weight of 0.027820975469831772\n",
      "MANU_Concentrator_Ct has weight of 0.028560512609912502\n",
      "expelled_per_c_num has weight of 0.02943368262964399\n",
      "ACTMath_ACTBenchmark_SWD has weight of 0.03286601871020469\n",
      "lea_avg_student_num has weight of 0.03332281502423644\n",
      "grade_range_cd_11-12 has weight of 0.033518704225690546\n",
      "EOCBiology_CACR_SWD has weight of 0.03489874800103656\n",
      "tchyrs_0thru3_pct has weight of 0.035425953136182714\n",
      "lateral_teach_pct has weight of 0.035638813815923744\n",
      "ACTWriting_ACTBenchmark_LEP has weight of 0.03619403574873576\n",
      "GraduationRate_4yr_Male has weight of 0.037401626449151196\n",
      "EOCBiology_CACR_Hispanic has weight of 0.03801879120131951\n",
      "Developing_TCHR_Standard 1_Pct has weight of 0.03844693504238745\n",
      "EVAAS Growth Score has weight of 0.03932401516548577\n",
      "ACTWorkKeys_SilverPlus_Hispanic has weight of 0.0404598104533981\n",
      "AAVC_Concentrator_Ct has weight of 0.04249237836202003\n",
      "ACTCompositeScore_UNCMin_Asian has weight of 0.04453690336805167\n",
      "SRC_Grades_Devices_Sent_Home_9:10 has weight of 0.04726093613538012\n",
      "SBE District_Northwest has weight of 0.04805866477798275\n",
      "Developing_TCHR_Standard 3_Pct has weight of 0.04811919999478749\n",
      "GraduationRate_5yr_Asian has weight of 0.04883986921111432\n",
      "Not Demostrated_TCHR_Standard 2_Pct has weight of 0.0494307194259247\n",
      "lea_wap_per_classroom has weight of 0.05125108935399198\n",
      "grade_range_cd_9-12 has weight of 0.05148571973981584\n",
      "BlackMalePct has weight of 0.05518515976265917\n",
      "ACTSubtests_BenchmarksMet_TwoorMoreRaces has weight of 0.057139609404554835\n",
      "ACTCompositeScore_UNCMin_EDS has weight of 0.05800087423084843\n",
      "EVAAS Growth Status_NotMet has weight of 0.06326664249205348\n",
      "ACTSubtests_BenchmarksMet_Asian has weight of 0.06360022015665863\n",
      "grades_BYOD_6:7:8:9:10:11:12:13 has weight of 0.06641888017253868\n",
      "ACTCompositeScore_UNCMin_AmericanIndian has weight of 0.06667229852702207\n",
      "lea_1yr_tchr_trnovr_pct has weight of 0.0669517117777873\n",
      "ACTWorkKeys_SilverPlus_Asian has weight of 0.06788721743738746\n",
      "ACTCompositeScore_UNCMin_TwoorMoreRaces has weight of 0.06929851404891353\n",
      "st_avg_student_num has weight of 0.06949722551416215\n",
      "EOCSubjects_CACR_Black has weight of 0.07270065430755818\n",
      "ACTCompositeScore_UNCMin_White has weight of 0.07661799192131166\n",
      "EOCBiology_CACR_Asian has weight of 0.07670772486101267\n",
      "SRC_Grades_Devices_Sent_Home_6:7:8:9:10:11:12 has weight of 0.07825526886214235\n",
      "grade_range_cd_K-12 has weight of 0.07852623490177484\n",
      "grade_range_cd_11-13 has weight of 0.07860575283522664\n",
      "Math SPG Grade_F has weight of 0.07940099876888171\n",
      "Math SPG Grade_B has weight of 0.079712450683945\n",
      "10_Size has weight of 0.08105421141346646\n",
      "grades_1_to_1_access_9:10:11:12 has weight of 0.08170403203661022\n",
      "ACTWriting_ACTBenchmark_EDS has weight of 0.08609804622091359\n",
      "ACTWorkKeys_SilverPlus_AIG has weight of 0.08700348810188417\n",
      "SBE District_Sandhills has weight of 0.08807187450358236\n",
      "grades_BYOD_11:12 has weight of 0.08877785182858125\n",
      "Accomplished_TCHR_Standard 4_Pct has weight of 0.08986922118988255\n",
      "TotalTargets_pTarget_PctMet has weight of 0.0905995156605587\n",
      "Distinguished_TCHR_Standard 2_Pct has weight of 0.09333617556464537\n",
      "EOCEnglish2_CACR_AIG has weight of 0.09376695848366061\n",
      "SBE District_Western has weight of 0.09409040926233878\n",
      "SRC_Grades_Devices_Sent_Home_10:11:12 has weight of 0.09424663862425081\n",
      "EOCSubjects_CACR_Asian has weight of 0.09781852370074516\n",
      "SRC_Grades_Devices_Sent_Home_8:9:10:11:12:13 has weight of 0.09944530828352349\n",
      "GraduationRate_5yr_White has weight of 0.10011515262424772\n",
      "ap_participation_pct has weight of 0.10268031691774\n",
      "SPG Score has weight of 0.1032400117264048\n",
      "EOCBiology_CACR_AIG has weight of 0.10455245124424195\n",
      "HOSP_Concentrator_Ct has weight of 0.10530894396712882\n",
      "_1yr_tchr_trnovr_pct has weight of 0.10857532181879717\n",
      "EOCSubjects_CACR_TwoorMoreRaces has weight of 0.10862348624812232\n",
      "Math I_Size has weight of 0.11045973657039848\n",
      "grades_BYOD_9:10:11:12:13 has weight of 0.11268264172028114\n",
      "ACTWriting_ACTBenchmark_SWD has weight of 0.11372102129688909\n",
      "ap_pct_3_or_above has weight of 0.11377820961494232\n",
      "IndianFemalePct has weight of 0.11507647577903952\n",
      "lea_tchyrs_4thru10_pct has weight of 0.11660432559416524\n",
      "lea_expelled_per_c_num has weight of 0.1196813108338117\n",
      "GraduationRate_4yr_White has weight of 0.12015576593203853\n",
      "GraduationRate_5yr_Male has weight of 0.12213336105056713\n",
      "English II_Size has weight of 0.12967978151217646\n",
      "ACTCompositeScore_UNCMin_Hispanic has weight of 0.1301779141695283\n",
      "ACTWorkKeys_SilverPlus_Black has weight of 0.13047136608292753\n",
      "GraduationRate_4yr_Female has weight of 0.1318419619550091\n",
      "ACTSubtests_BenchmarksMet_Hispanic has weight of 0.13251925210783622\n",
      "Reading SPG Grade_F has weight of 0.13496336375040233\n",
      "Biology Score has weight of 0.1379464459469681\n",
      "EOCMathI_CACR_Black has weight of 0.1391442037480885\n",
      "EOCSubjects_CACR_AmericanIndian has weight of 0.1409709239225148\n",
      "GraduationRate_5yr_All has weight of 0.1418988761087985\n",
      "AGNR_Concentrator_Ct has weight of 0.14291898926784724\n",
      "SPG Grade_A+NG has weight of 0.1438189416934956\n",
      "SBE District_Northeast has weight of 0.14619146753487863\n",
      "sat_participation_pct has weight of 0.1483172891938426\n",
      "lea_short_susp_per_c_num has weight of 0.15395614786631964\n",
      "lea_avg_daily_attend_pct has weight of 0.15412782329441163\n",
      "sat_avg_score_num has weight of 0.1549785702842993\n",
      "ACTSubtests_BenchmarksMet_LEP has weight of 0.15544941603053442\n",
      "ACTCompositeScore_UNCMin_AIG has weight of 0.16151609299566055\n",
      "lea_stud_internet_comp_num has weight of 0.16260028030990942\n",
      "ACT Score has weight of 0.16826013589796182\n",
      "Accomplished_TCHR_Standard 1_Pct has weight of 0.1683526121298713\n",
      "avg_daily_attend_pct has weight of 0.16947764054712133\n",
      "Grad_project_status_Y has weight of 0.1738633774270657\n",
      "lea_salary_expense_pct has weight of 0.17817964407711304\n",
      "ACTCompositeScore_UNCMin_LEP has weight of 0.17974644941482817\n",
      "GraduationRate_5yr_TwoorMoreRaces has weight of 0.18056142949837897\n",
      "SBE District_Southeast has weight of 0.1842517225632116\n",
      "grade_range_cd_PK-13 has weight of 0.18840930987178983\n",
      "EOCBiology_CACR_Black has weight of 0.19680890506617738\n",
      "tchyrs_11plus_pct has weight of 0.2011731875873593\n",
      "EOCEnglish2_CACR_Hispanic has weight of 0.2193523387988055\n",
      "GraduationRate_5yr_AIG has weight of 0.22395567089683285\n",
      "EOCMathI_CACR_AIG has weight of 0.22447514111267775\n"
     ]
    }
   ],
   "source": [
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(xTrain) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(xTrain) # apply to training\n",
    "X_test_scaled = scl_obj.transform(xTest) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,yTrain)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(yTest,y_hat)\n",
    "conf = mt.confusion_matrix(yTest,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,X.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "still 401 explanatory variables at the moment, need to figure out how to knock a lot out. L1 (lasso regression) gave about 2% increase in accuracy compared to L2 (ridge regression). but that could just be due to randomness if we re-ran them a few more times. also need to figure out GridSearchCV for the costs on this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zip_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models (50 points)\n",
    "<span style=\"color: blue\">Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). <b>Adjust parameters of the models to make them more accurate.</b> If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column preparation\n",
    "\n",
    "At this point, the dataset contains 'future' information that the model should not be aware of. We need to drop these columns before we build the model. In addition, some of the column names don't start with alpha characters which cause code later in the notebook to miss these columns and not graph the most significant support vectors correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: https://stackoverflow.com/questions/19758364/rename-a-single-column-header-in-a-pandas-dataframe\n",
    "# Credit: https://stackoverflow.com/questions/21606987/how-can-i-strip-the-whitespace-from-pandas-dataframe-headers\n",
    "# Credit: https://stackoverflow.com/questions/39741429/pandas-replace-a-character-in-all-column-names\n",
    "# Credit: https://stackoverflow.com/questions/33157643/pandas-replace-erase-different-characters-from-strings\n",
    "\n",
    "# Drop these columns because they contain the answer we are trying to predict\n",
    "dropCols = dfPublicHS.filter(regex = r'sch_pct|college')\n",
    "\n",
    "# Drop columns\n",
    "dfPublicHS.drop(dropCols, axis = 1, inplace = True)\n",
    "\n",
    "# These names seem to cause problems so let's give them friendlier names\n",
    "renameCols = {'_1yr_tchr_trnovr_pct': 'One_yr_tchr_trnovr_pct',\n",
    "              '0-3 Years_LEA_Exp_Pct_Prin': 'less_3_years_LEA_Exp_Pct_Prin',\n",
    "              '10+ Years_LEA_Exp_Pct_Prin': 'ten_plus_years_LEA_Exp_Pct_Prin',\n",
    "              '4-10 Years_LEA_Exp_Pct_Prin': 'four_plus_years_LEA_Exp_Pct_Prin',\n",
    "              '4-Year_Cohort_Graduation_Rate_Score': 'four_Year_Cohort_Graduation_Rate_Score',\n",
    "              '_1_to_1_access_Yes': 'one_to_one_access_yes'}\n",
    "\n",
    "# Remove any trailing white spaces\n",
    "dfPublicHS.columns = dfPublicHS.columns.str.strip()\n",
    "\n",
    "# Remove any spaces, slashes, or hyphens\n",
    "dfPublicHS.columns = dfPublicHS.columns.str.replace(r' |/|-', \"_\")\n",
    "\n",
    "# Rename columns\n",
    "dfPublicHS.rename(columns=renameCols, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Features\n",
    "For comparison against the two models, we'll build a model with the attributes used for the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(C=[1,10,100,1000])\n",
    "gridLinear = GridSearchCV(SVC(), param_grid=param_grid, iid = False, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at: 18:13:57.474234\n",
      "Wall time: 2.39 s\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10}\n",
      "\n",
      "gridLinear scores on development set:\n",
      "\n",
      "0.691 (+/-0.090) for {'C': 1}\n",
      "0.723 (+/-0.103) for {'C': 10}\n",
      "0.723 (+/-0.117) for {'C': 100}\n",
      "0.723 (+/-0.117) for {'C': 1000}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train SVM model\n",
    "print(\"Training started at: {}\".format(datetime.now().time()))\n",
    "%time gridLinear.fit(xTrainLR, yTrainLR)\n",
    "\n",
    "bestValues = gridLinear.best_params_\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(bestValues)\n",
    "print()\n",
    "print(\"gridLinear scores on development set:\")\n",
    "print()\n",
    "means = gridLinear.cv_results_['mean_test_score']\n",
    "stds = gridLinear.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, gridLinear.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29 24]\n",
      " [16 25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.55      0.59        53\n",
      "           1       0.51      0.61      0.56        41\n",
      "\n",
      "   micro avg       0.57      0.57      0.57        94\n",
      "   macro avg       0.58      0.58      0.57        94\n",
      "weighted avg       0.59      0.57      0.58        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set model to best values found\n",
    "cls = SVC(C = bestValues['C'], kernel = 'linear')\n",
    "\n",
    "# Train\n",
    "cls.fit(xTrainLR, yTrainLR)\n",
    "\n",
    "# Predict test values\n",
    "yhat = cls.predict(xTestLR)\n",
    "\n",
    "# Show model performance\n",
    "print(confusion_matrix(yTest, yhat))\n",
    "print(classification_report(yTest, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Using the ANOVA F-value to determine which columns have the greatest proportion of variance, we select the top 10% to be used in our model. The F-value tells us which column(s) help to explain the greatest amount of variance in the target variable. The columns with the highest proportion allows the model to solve for the greatest separation on attributes for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data to exclude the target variable\n",
    "maxCols = dfPublicHS.shape[1]\n",
    "X = dfPublicHS.iloc[:,0:maxCols-2]\n",
    "\n",
    "# Choose the top 10% attributes based on ANOVA score\n",
    "sel = SelectPercentile(f_classif, percentile = 10)\n",
    "sel.fit(X, y)\n",
    "\n",
    "# Credit: https://www.kaggle.com/saravanann/santander\n",
    "# Convert back to dataframe\n",
    "\n",
    "keepCols = sel.get_support(indices=True)\n",
    "temp = sel.transform(X)\n",
    "X = pd.DataFrame(temp, columns = X.columns[keepCols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split\n",
    "Split the resulting dataset into 80% for training and 20% for testing. After we split the training data, we then scale the data so that each column contributes equal weight to the model. A column with a range of 1 and a column with a range of 10,000 will be highly biased by the column with a range of 10,000. Scaling the columns puts each column on an equal range. We use fit to calculate the mean and standard deviation for the train data set. We use the mean and standard deviation of the train data on the test data so that information isn't biased by data that we shouldn't know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test and train dataset with an 80/20 split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X.astype(np.float), y, test_size = 0.2)\n",
    "\n",
    "# Scale data after splitting out test data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(xTrain)\n",
    "xTrain = scaler.transform(xTrain)\n",
    "xTest = scaler.transform(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning\n",
    "Here we create a range of values to test the model against to determine which parameters provide the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter turning values and create a grid of models to run\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=[1,10,100,1000])\n",
    "\n",
    "gridRBF = GridSearchCV(SVC(), param_grid=param_grid, iid = False, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM model on the each different parameter value\n",
    "print(\"Training started at: {}\".format(datetime.now().time()))\n",
    "%time gridRBF.fit(xTrain, yTrain)\n",
    "\n",
    "bestValues = gridRBF.best_params_\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(bestValues)\n",
    "print()\n",
    "print(\"gridRBF scores on development set:\")\n",
    "print()\n",
    "means = gridRBF.cv_results_['mean_test_score']\n",
    "stds = gridRBF.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, gridRBF.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model\n",
    "Using the best parameters found from the grid search above, test our model against data that it hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set model to best values found\n",
    "cls = SVC(C = bestValues['C'], gamma = bestValues['gamma'])\n",
    "\n",
    "# Train\n",
    "cls.fit(xTrain, yTrain)\n",
    "\n",
    "# Predict test values\n",
    "yhat = cls.predict(xTest)\n",
    "\n",
    "# Show model performance\n",
    "print(confusion_matrix(yTest, yhat))\n",
    "print(classification_report(yTest, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "PLot an ROC curve of the results. An ROC curve helps show the trade offs between true-negatives and false-positives. For instance, if we were trying to predict whether someone has a deadly illness then we would want to select a model that gives the fewest number of true-negatives. However, in our cause, it's acceptable to have a higher false-positive rate since we are only exploring attributes that could contribute to higher levels of students enrolling in postsecondary education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an ROC Graph\n",
    "fpr, tpr, thresholds = roc_curve(yTest, yhat)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most influential Support Vectors\n",
    "The graph below helps us determine which attributes have the most impact on the boundary between support vectors. This gives us some indication of what attributes might help increase the number of students enrolled in postsecondary education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the columns in terms of prediction weight\n",
    "cv = CountVectorizer()\n",
    "cv.fit(X)\n",
    "\n",
    "plot_coefficients(cls, cv.get_feature_names(), top_features = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning\n",
    "Here we create a range of values to test the model against to determine which parameters provide the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(C=[1,10,100,1000])\n",
    "gridLinear = GridSearchCV(SVC(), param_grid=param_grid, iid = False, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM model\n",
    "print(\"Training started at: {}\".format(datetime.now().time()))\n",
    "%time gridLinear.fit(xTrain, yTrain)\n",
    "\n",
    "bestValues = gridLinear.best_params_\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(bestValues)\n",
    "print()\n",
    "print(\"gridLinear scores on development set:\")\n",
    "print()\n",
    "means = gridLinear.cv_results_['mean_test_score']\n",
    "stds = gridLinear.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, gridLinear.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model\n",
    "Using the best parameters found from the grid search above, test our model against data that it hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = SVC(C = bestValues['C'], kernel = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "cls.fit(xTrain, yTrain)\n",
    "\n",
    "# Predict test values\n",
    "yhat = cls.predict(xTest)\n",
    "\n",
    "# Show model performance\n",
    "print(confusion_matrix(yTest, yhat))\n",
    "print(classification_report(yTest, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "PLot an ROC curve of the results. An ROC curve helps show the trade offs between true-negatives and false-positives. For instance, if we were trying to predict whether someone has a deadly illness then we would want to select a model that gives the fewest number of true-negatives. However, in our cause, it's acceptable to have a higher false-positive rate since we are only exploring attributes that could contribute to higher levels of students enrolling in postsecondary education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an ROC Graph\n",
    "fpr, tpr, thresholds = roc_curve(yTest, yhat)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most influential Support Vectors\n",
    "The graph below helps us determine which attributes have the most impact on the boundary between support vectors. This gives us some indication of what attributes might help increase the number of students enrolled in postsecondary education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Source: https://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d\n",
    "\n",
    "# Graph the columns in terms of prediction weight\n",
    "cv = CountVectorizer()\n",
    "cv.fit(X)\n",
    "\n",
    "plot_coefficients(cls, cv.get_feature_names(), top_features = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cool Plot\n",
    "# http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Advantages (10 points)\n",
    "<span style=\"color: blue\">Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Feature Importance (30 points)\n",
    "\n",
    "<span style=\"color: blue\">Use the weights from logistic regression to interpret the importance of different features for the classification task. <b>Explain your interpretation in detail.</b>Why do you think some variables are more important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Support Vectors (10 points)\n",
    "<span style=\"color:blue\">Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model— then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
